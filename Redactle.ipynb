{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redactle\n",
    "\n",
    "Train a decision tree to narrow down the topic of an arbitrary wikipedia article, motivated by the game [redactle](redactle.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import bs4\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import pathlib\n",
    "import requests\n",
    "import collections\n",
    "import urllib.parse\n",
    "import sklearn.tree\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_URL = 'https://en.wikipedia.org/'\n",
    "DATA_DIR = pathlib.Path('articles')\n",
    "assert DATA_DIR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('articles.json', 'r') as infile:\n",
    "    articles_data = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_leaves(tree_dict):\n",
    "    if 'children' not in tree_dict.keys():\n",
    "        return [tree_dict['id']]\n",
    "    leaves = []\n",
    "    for child in tree_dict['children']:\n",
    "        leaves.extend(get_all_leaves(child))\n",
    "    return leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = get_all_leaves(articles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org//wiki/Julie_Andrews'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_URL + leaves[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_by_category(articles):\n",
    "    categories = {}\n",
    "    for category in articles['children']:\n",
    "        categories[category['name']] = get_all_leaves(category)\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = get_articles_by_category(articles_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_articles(articles):\n",
    "    for label, url_paths in articles.items():\n",
    "        os.makedirs(DATA_DIR / label)\n",
    "        for url_path in url_paths:\n",
    "            time.sleep(0.1)\n",
    "            request = requests.get(ROOT_URL + url_path)\n",
    "            if request.status_code == 200:\n",
    "                html = request.content\n",
    "                title = os.path.split(url_path)[-1]\n",
    "                with open(DATA_DIR / label / f'{title}.html', 'wb') as outfile:\n",
    "                    outfile.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download_articles(get_articles_by_category(articles_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(html):\n",
    "    soup = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    content = soup.find('div', id='content').find('div', id='bodyContent').find('div', id='mw-content-text').find('div')\n",
    "    if content.find(class_='shortdescription'): content.find(class_='shortdescription').decompose()\n",
    "    if content.find(id='References'): content.find(id='References').decompose()\n",
    "    if content.find(class_='reflist'): content.find(class_='reflist').decompose()\n",
    "    for i in content.find_all(class_='mwe-math-element'): i.decompose()\n",
    "    text = '\\n'.join([i.get_text() for i in content if i.name == 'p' or i.name == 'h1' or i.name == 'h2' or i.name == 'h3' or i.name == 'ul'])\n",
    "    text = text.split('See also[edit]')[0]\n",
    "    text = re.sub('[\\(\\[].*?[\\)\\]]', '', text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digest_articles(directory):\n",
    "    for article in os.listdir(directory):\n",
    "        if os.path.splitext(article)[-1] != '.html': continue\n",
    "        with open(os.path.join(directory, article), 'rb') as infile:\n",
    "            html = infile.read()\n",
    "        text = extract_text(html)\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [token for token in tokens if not any([(punc in token) for punc in string.punctuation])]\n",
    "        counts = collections.Counter(tokens)\n",
    "        filename = os.path.splitext(article)[0]+'.txt'\n",
    "        with open(os.path.join(directory, filename), 'w') as outfile:\n",
    "            outfile.write(f'ALL_WORDS:{sum(counts.values())}\\n')\n",
    "            title = os.path.splitext(article)[0]\n",
    "            outfile.write(f'TITLE_NUM_WORDS:{len(title.split(\"_\"))}\\n')\n",
    "            for i in range(5):\n",
    "                if i < len(title.split('_')):\n",
    "                    w = title.split('_')[i]\n",
    "                    outfile.write(f'TITLE_WORD_{i}_LEN:{len(w)}\\n')\n",
    "            for word, count in counts.items():\n",
    "                if count > 1:\n",
    "                    outfile.write(f'{word}:{count}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [os.path.join(DATA_DIR, category) for category in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, category))]\n",
    "\n",
    "with multiprocessing.Pool(len(categories)) as p:\n",
    "    p.map(digest_articles, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(directory):\n",
    "    words = collections.Counter()\n",
    "    for category in os.listdir(directory):\n",
    "        for article in os.listdir(os.path.join(directory, category)):\n",
    "            if os.path.splitext(article)[-1] != '.txt': continue\n",
    "            with open(os.path.join(directory, category, article), 'r') as infile:\n",
    "                data = infile.read()\n",
    "            for line in data.split('\\n'):\n",
    "                if line == '': continue\n",
    "                word = line.split(':')[0]\n",
    "                words[word] += 1\n",
    "    words_array = np.array([word for word, count in words.items() if count > 5]) # optimized for getting word of index\n",
    "    words_dict = {word:i for i, word in enumerate(words_array)} # optimized for getting index of word\n",
    "    return words_array, words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_array, words_dict = load_word_list('articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words.pkl', 'wb') as outfile:\n",
    "    pickle.dump([words_array, words_dict], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('words.pkl', 'rb') as infile:\n",
    "    words_array, words_dict = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39620"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(min_count=100, max_label_depth=3):\n",
    "    \n",
    "    nleaves = len(leaves)\n",
    "    features = np.zeros((nleaves, len(words_array)))\n",
    "    labels = np.zeros(nleaves)\n",
    "    label_counts = [ ]\n",
    "    all_labels = [ ]\n",
    "    idx = 0\n",
    "    top = None\n",
    "    tpath = [ ]\n",
    "    \n",
    "    def scan(node, depth, label):\n",
    "        nonlocal idx, top,  tpath\n",
    "        saved_tpath = list(tpath)\n",
    "        if depth == 1:\n",
    "            top = node['name']\n",
    "            tpath = [ top ]\n",
    "            label = len(all_labels)\n",
    "            all_labels.append(node['name'])\n",
    "            label_counts.append(0)\n",
    "        elif (depth > 1) and (depth <= max_label_depth) and (node.get('count',0) >= min_count):\n",
    "            label = len(all_labels)\n",
    "            tpath.append(node['name'])\n",
    "            all_labels.append(' / '.join(tpath))\n",
    "            label_counts.append(0)\n",
    "            \n",
    "        children = node.get('children', [])\n",
    "        if children:\n",
    "            for child in node['children']:\n",
    "                scan(child, depth + 1, label)\n",
    "        else:\n",
    "            name = node[\"id\"][6:]\n",
    "            if '/' in name:\n",
    "                # This only occurs for \"HIV/AIDS\"\n",
    "                name = name[:name.index('/')]\n",
    "            path = DATA_DIR / top / f'{name}.txt'\n",
    "            with open(path, 'r') as infile:\n",
    "                data = infile.read()\n",
    "            for line in data.split('\\n'):\n",
    "                if line == '': continue\n",
    "                word, count = line.split(':')\n",
    "                count = int(count)\n",
    "                if word in words_dict:\n",
    "                    features[idx, words_dict[word]] = count\n",
    "            labels[idx] = label\n",
    "            label_counts[label] += 1\n",
    "            idx += 1\n",
    "        tpath = saved_tpath\n",
    "\n",
    "    scan(articles_data, 0, None)\n",
    "    return features, labels, all_labels, label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, all_labels, label_counts = generate_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'People': 422,\n",
       " 'People / Visual artists': 124,\n",
       " 'People / Writers': 79,\n",
       " 'People / Writers / Late modern': 176,\n",
       " 'People / Musicians and composers': 149,\n",
       " 'People / Philosophers, historians, political and social scientists': 162,\n",
       " 'People / Religious figures': 124,\n",
       " 'People / Politicians and leaders': 72,\n",
       " 'People / Politicians and leaders / Post-classical': 125,\n",
       " 'People / Politicians and leaders / Early modern period': 116,\n",
       " 'People / Politicians and leaders / Late modern period': 187,\n",
       " 'People / Scientists, inventors and mathematicians': 254,\n",
       " 'History': 253,\n",
       " 'History / Ancient history': 127,\n",
       " 'History / Post-classical history': 133,\n",
       " 'History / Late modern history': 171,\n",
       " 'Geography': 54,\n",
       " 'Geography / Physical geography': 192,\n",
       " 'Geography / Physical geography / Bodies of water': 189,\n",
       " 'Geography / Countries': 207,\n",
       " 'Geography / Regions and country subdivisions': 111,\n",
       " 'Geography / Cities': 274,\n",
       " 'Geography / Cities / Asia': 177,\n",
       " 'Arts': 208,\n",
       " 'Arts / Literature': 56,\n",
       " 'Arts / Literature / Specific works': 162,\n",
       " 'Arts / Music': 137,\n",
       " 'Arts / Visual arts': 109,\n",
       " 'Philosophy and religion': 229,\n",
       " 'Philosophy and religion / Philosophy': 101,\n",
       " 'Philosophy and religion / Mythology': 108,\n",
       " 'Everyday life': 147,\n",
       " 'Everyday life / Cooking, food and drink': 119,\n",
       " 'Everyday life / Sports and recreation': 69,\n",
       " 'Everyday life / Sports and recreation / Sports': 141,\n",
       " 'Society and social sciences': 607,\n",
       " 'Society and social sciences / Business and economics': 122,\n",
       " 'Society and social sciences / Language': 197,\n",
       " 'Biology and health sciences': 209,\n",
       " 'Biology and health sciences / Anatomy and morphology': 121,\n",
       " 'Biology and health sciences / Organisms': 44,\n",
       " 'Biology and health sciences / Organisms / Animals': 567,\n",
       " 'Biology and health sciences / Organisms / Plants': 259,\n",
       " 'Biology and health sciences / Health, medicine and disease': 144,\n",
       " 'Biology and health sciences / Health, medicine and disease / Morbidity': 137,\n",
       " 'Physical sciences': 80,\n",
       " 'Physical sciences / Astronomy': 195,\n",
       " 'Physical sciences / Chemistry': 52,\n",
       " 'Physical sciences / Chemistry / Chemical substances': 220,\n",
       " 'Physical sciences / Earth science': 98,\n",
       " 'Physical sciences / Earth science / Earth': 163,\n",
       " 'Physical sciences / Physics': 293,\n",
       " 'Technology': 638,\n",
       " 'Technology / Industry': 100,\n",
       " 'Mathematics': 301}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(all_labels, label_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features-new.pkl', 'wb') as outfile:\n",
    "    pickle.dump([features, labels, all_labels], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features-new.pkl', 'rb') as infile:\n",
    "    features, labels, all_labels = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.tree.DecisionTreeClassifier(max_depth = 20, criterion='entropy') #, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 128 ms, total: 26.6 s\n",
      "Wall time: 26.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=20, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time classifier.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9754270302667066"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['father' 'species' 'population' 'used' 'city' 'death' 'career' 'economy'\n",
      " 'music' 'military' 'earth' 'people' 'world' 'works' 'art' 'language'\n",
      " 'political' 'may' 'genus' 'god' 'government' 'united' 'work' 'include'\n",
      " 'designed' 'production' 'cells' 'energy' 'north' 'area' 'games' 'played'\n",
      " 'climate' 'numbers' 'example' 'literary' 'reign' 'chemical' 'water'\n",
      " 'TITLE_WORD_1_LEN' 'century' 'treatment' 'ALL_WORDS' 'life' 'country'\n",
      " 'mathematical' 'family' 'december' 'one' 'compounds' 'theory'\n",
      " 'philosophy' 'food' 'common' 'stars' 'symptoms' 'many' 'king'\n",
      " 'university' 'leaves' 'southern' 'published' 'grown' 'TITLE_WORD_0_LEN'\n",
      " 'human' 'use' 'years' 'later' 'first' 'spoken' 'science' 'often'\n",
      " 'scientific' 'plants' 'body' 'early' 'term' 'bc' 'also' 'including'\n",
      " 'story' 'religious' 'army' 'organisms' 'time' 'known' 'languages'\n",
      " 'humans' 'period' 'high' 'TITLE_NUM_WORDS' 'large' 'defined' 'usually'\n",
      " 'popular' 'national' 'made' 'person' 'around' 'systems' 'ancient' 'form'\n",
      " 'unit' 'index' 'surface' 'like' 'could' 'gods' 'american' 'two' 'novels'\n",
      " 'english' 'part' 'sea' 'new' 'state' 'writer' 'disease' 'according'\n",
      " 'within' 'de' 'females' 'metals' 'british' 'using' 'rock' 'classical'\n",
      " 'certain' 'object' 'social' 'february' 'particles' 'medical' 'research'\n",
      " 'process' 'males' 'called' 'effects' 'europe' 'born' 'dynasty' 'physics'\n",
      " 'name' 'black' 'literature' 'mass' 'war' 'among' 'august' 'mathematics'\n",
      " 'based' 'olympic' 'died' 'artists' 'african' 'power' 'island' 'â€“'\n",
      " 'character' 'figures' 'style' 'caused' 'constitution' 'western'\n",
      " 'produced' 'modern' 'theorem' 'religion' 'low' 'writing' 'sometimes'\n",
      " 'acid' 'largest' 'small' 'found' 'major' 'molecules' 'point' 'several'\n",
      " 'west' 'areas' 'given' 'east' 'matter' 'took' 'christian' 'structure'\n",
      " 'level' 'result' 'control' 'film' 'system' 'TITLE_WORD_2_LEN' 'stage'\n",
      " 'specific' 'april' 'written' 'would' 'history' 'basin']\n"
     ]
    }
   ],
   "source": [
    "top_features = np.argsort(classifier.feature_importances_)[::-1][:200]\n",
    "print(words_array[top_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree2json(tree, name):\n",
    "    data = dict(\n",
    "        classes = all_labels,\n",
    "        words = [words_array[i] if i >= 0 else \"\" for i in tree.feature],\n",
    "        cuts = [int(i) if i >= 0 else 0 for i in tree.threshold],\n",
    "        below = tree.children_left.tolist(),\n",
    "        above = tree.children_right.tolist(),\n",
    "        probs = [p[0].astype(int).tolist() for p in tree.value],\n",
    "    )\n",
    "    with open(name, 'w') as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2json(classifier.tree_, 'tree20-new-entropy-100-3.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
